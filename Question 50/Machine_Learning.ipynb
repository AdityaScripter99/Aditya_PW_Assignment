{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.old_function()>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def old_function():\n",
    "    warnings.warn(\"This is deprecated\", DeprecationWarning)\n",
    "\n",
    "old_function    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Difference between Series & DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -->Series: A one-dimensional labeled array capable of holding any data type. It is like a column in a table.\n",
    "\n",
    "# -->DataFrame: A two-dimensional labeled data structure with columns of potentially different types. \n",
    "#               It is like a table with rows and columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Create a Database, Table, and Use Pandas to Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"your_username\",\n",
    "    password=\"your_password\",\n",
    "    database=\"Travel_Planner\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bookings (\n",
    "        user_id INT,\n",
    "        flight_id INT,\n",
    "        hotel_id INT,\n",
    "        activity_id INT,\n",
    "        booking_date DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert dummy data\n",
    "dummy_data = [\n",
    "    (1, 101, 201, 301, '2024-09-01'),\n",
    "    (2, 102, 202, 302, '2024-09-02'),\n",
    "    (3, 103, 203, 303, '2024-09-03'),\n",
    "    (4, 104, 204, 304, '2024-09-04'),\n",
    "    (5, 105, 205, 305, '2024-09-05')\n",
    "]\n",
    "\n",
    "# Insert multiple rows at once\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO bookings (user_id, flight_id, hotel_id, activity_id, booking_date) \n",
    "    VALUES (%s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "cursor.executemany(insert_query, dummy_data)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Fetch and display the data using pandas\n",
    "cursor.execute(\"SELECT * FROM bookings\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "df = pd.DataFrame(rows, columns=['user_id', 'flight_id', 'hotel_id', 'activity_id', 'booking_date'])\n",
    "print(df)\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Difference between loc and iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc: Access a group of rows and columns by labels or a boolean array.\n",
    "\n",
    "# iloc: Access a group of rows and columns by integer positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Difference between Supervised and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning: The model is trained on a labeled dataset, meaning that each training example is paired with an output label.\n",
    "\n",
    "# Unsupervised Learning: The model is trained on an unlabeled dataset and must find patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Explain the Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias: Error due to overly simplistic assumptions in the learning algorithm, leading to underfitting.\n",
    "\n",
    "# Variance: Error due to the model being too complex and fitting the noise in the training data, leading to overfitting.\n",
    "\n",
    "# Tradeoff: There’s a tradeoff between bias and variance. Increasing bias decreases variance and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Precision and Recall, and Difference from Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision: The ratio of true positive predictions to the total predicted positives.\n",
    "\n",
    "# Recall: The ratio of true positive predictions to the total actual positives.\n",
    "\n",
    "# Accuracy: The ratio of correctly predicted instances (both true positives and true negatives) to the total instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.What is Overfitting and How Can It Be Prevented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting: A model performs well on the training data but poorly on unseen data because it has learned the noise in the training data.\n",
    "\n",
    "# Prevention: Can be prevented by techniques like cross-validation, pruning, early stopping, regularization, and using simpler models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Explain the Concept of Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation: A technique used to assess the generalization ability of a model by splitting the dataset into training and testing subsets multiple times. \n",
    "#                   A common method is k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awesome_dreamer_adit/.local/lib/python3.10/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96666667 0.96666667 0.93333333 0.93333333 1.        ]\n"
     ]
    }
   ],
   "source": [
    "#Program:\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Example of k-fold cross-validation\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Cross-validation scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.Difference between Classification and Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: Predicts a discrete label (e.g., spam or not spam).\n",
    "\n",
    "# Regression: Predicts a continuous value (e.g., house prices).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Explain the Concept of Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning: Combines predictions from multiple models to improve performance. Examples include Bagging (e.g., Random Forest) and Boosting (e.g., Gradient Boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.What is Gradient Descent and How Does it Work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent: An optimization algorithm used to minimize the loss function by iteratively moving towards the steepest descent direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: [4.23281554 2.70222808]\n"
     ]
    }
   ],
   "source": [
    "#Program:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example of gradient descent for a simple linear regression\n",
    "def gradient_descent(X, y, lr=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(epochs):\n",
    "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
    "        theta -= lr * gradients\n",
    "    return theta\n",
    "\n",
    "# Generate random data\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Reshape y to a 1D array for proper matrix multiplication\n",
    "y = y.ravel()\n",
    "\n",
    "theta = gradient_descent(X_b, y)\n",
    "print(\"Parameters:\", theta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.Batch Gradient Descent vs. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent: Computes the gradient using the entire dataset.\n",
    "\n",
    "# Stochastic Gradient Descent (SGD): Computes the gradient using a single sample or a mini-batch, making it faster but more noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.The Curse of Dimensionality in Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curse of Dimensionality: As the number of features increases, the volume of the feature space grows exponentially, making the data sparse and models less effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.Difference between L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Regularization (Lasso): Adds the absolute value of the coefficients to the loss function, leading to sparse models.\n",
    "\n",
    "# L2 Regularization (Ridge): Adds the square of the coefficients to the loss function, leading to small, non-zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.Confusion Matrix and Its Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix: A table used to describe the performance of a classification model by showing the actual vs. predicted classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2 1]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "#Program:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [0, 1, 1, 0, 1, 0]\n",
    "y_pred = [0, 1, 0, 0, 1, 1]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.Define AUC-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC Curve: The Area Under the Receiver Operating Characteristic Curve, which measures the model's ability to distinguish between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Program:\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = [0, 1, 1, 0, 1, 0]\n",
    "y_scores = [0.1, 0.9, 0.8, 0.4, 0.7, 0.2]\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(\"AUC-ROC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.Explain the k-Nearest Neighbors Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors (k-NN): A simple algorithm that classifies a data point based on the majority label of its k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.Basic Concept of a Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM: A classification algorithm that finds the hyperplane that best separates the data into different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.How Does the Kernel Trick Work in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Trick: Allows SVM to operate in a high-dimensional space without explicitly computing the coordinates by using kernel functions (e.g., linear, polynomial, RBF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.Different Types of Kernels Used in SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Kernel: Used for linearly separable data.\n",
    "\n",
    "# Polynomial Kernel: Used when the data is not linearly separable but can be separated by a polynomial.\n",
    "\n",
    "# RBF Kernel: Maps the data into a higher-dimensional space to make it linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.What is the Hyperplane in SVM and How is it Determined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperplane: The decision boundary that separates different classes in SVM. It’s determined by maximizing the margin between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.Pros and Cons of Using a Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pros: Effective in high-dimensional spaces, robust to overfitting.\n",
    "# Cons: Not efficient for large datasets, requires careful tuning of parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.Hard Margin vs. Soft Margin in SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Margin SVM: No tolerance for misclassification, used when data is perfectly separable.\n",
    "# Soft Margin SVM: Allows some misclassification, used when data is not perfectly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.Process of Constructing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a Decision Tree: The process involves selecting the best attribute (using criteria like Gini impurity, Information gain) and recursively splitting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.Working Principle of a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree: A model that splits the data based on feature values to create a tree where each node represents a decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26.Information Gain and Its Use in Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Gain: Measures the reduction in entropy when the dataset is split based on a feature. It’s used to decide which feature to split on at each step in constructing a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27.Gini Impurity and Its Role in Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Impurity: A metric to measure how often a randomly chosen element would be incorrectly classified if it was randomly labeled according to the distribution of labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program:\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# # Example of decision tree with Gini impurity\n",
    "# model = DecisionTreeClassifier(criterion=\"gini\")\n",
    "# model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.Advantages and Disadvantages of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages:\n",
    "\n",
    "# Easy to understand and interpret.\n",
    "# Can handle both numerical and categorical data.\n",
    "# Non-parametric and doesn’t require scaling of data.\n",
    "\n",
    "# Disadvantages:\n",
    "\n",
    "# Prone to overfitting, especially with noisy data.\n",
    "# Can create biased trees if some classes dominate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29.How Do Random Forests Improve Upon Decision Trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests: Combine multiple decision trees to improve accuracy and reduce overfitting by averaging the results. They also use bootstrapping and feature randomness to ensure trees are diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30.How Does a Random Forest Algorithm Work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest:\n",
    "# Creates a \"forest\" of decision trees using bootstrapped subsets of the data.\n",
    "\n",
    "# Randomly selects a subset of features to split at each node.\n",
    "\n",
    "# Aggregates the results from each tree to make a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: [0.13344079 0.02397317 0.39478296 0.44780308]\n"
     ]
    }
   ],
   "source": [
    "#Program:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Example: Random Forest Classifier\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Feature Importances:\", model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31.Bootstrapping in the Context of Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping: Random sampling with replacement to create multiple datasets from the original one. Each tree in the random forest is trained on a different bootstrapped dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32.Explain the Concept of Feature Importance in Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance: Measures how much a given feature contributes to the decision-making process in a model. In Random Forests, it is often calculated based on how much each feature decreases the weighted impurity in a tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33.Key Hyperparameters of a Random Forest and How They Affect the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators: Number of trees in the forest.\n",
    "# max_depth: Maximum depth of each tree.\n",
    "# min_samples_split: Minimum number of samples required to split an internal node.\n",
    "# max_features: Number of features to consider when looking for the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 34.Describe the Logistic Regression Model and Its Assumptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: A linear model used for binary classification that models the probability of a class using a logistic function.\n",
    "# Assumptions:\n",
    "# Linearity in the logit (log-odds).\n",
    "# Independence of errors.\n",
    "# No multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 35.How Does Logistic Regression Handle Binary Classification Problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification: Logistic regression outputs a probability for the positive class. A threshold (commonly 0.5) is applied to decide the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program:\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Example: Logistic Regression for binary classification\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X, y)\n",
    "\n",
    "# print(\"Model Coefficients:\", model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 36.What is the Sigmoid Function and How is it Used in Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function: A mathematical function that maps input values to a range between 0 and 1. Used in logistic regression to model probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Output: [0.11920292 0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "#Program:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.array([-2, 0, 2])\n",
    "print(\"Sigmoid Output:\", sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 37.Concept of the Cost Function in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function: Measures how well the model's predictions match the true labels. In logistic regression, it's the log-loss (binary cross-entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 38.Extending Logistic Regression to Handle Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Logistic Regression: Uses one-vs-rest or softmax for multi-class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program:\n",
    "\n",
    "# Multiclass Logistic Regression example using softmax\n",
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "# model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 39.Difference between L1 and L2 Regularization in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Regularization: Adds the absolute value of coefficients to the cost function (leads to sparse models).\n",
    "# L2 Regularization: Adds the square of coefficients to the cost function (leads to smaller coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40.What is XGBoost and How Does it Differ from Other Boosting Algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost: An efficient implementation of gradient boosting with additional features like regularization and handling missing values.\n",
    "# Difference: XGBoost provides regularization terms, early stopping, and optimized speed compared to traditional boosting algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program:\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # Example: XGBoost Classifier\n",
    "# model = XGBClassifier()\n",
    "# model.fit(X, y)\n",
    "\n",
    "# print(\"XGBoost Model:\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41.Explain the Concept of Boosting in the Context of Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting: An ensemble technique that sequentially builds models, each one correcting the errors of the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42.How Does XGBoost Handle Missing Values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values: XGBoost automatically learns the best direction (left or right) to take when it encounters a missing value in a feature during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43.Key Hyperparameters in XGBoost and How They Affect Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate: Controls the contribution of each tree to the final prediction.\n",
    "# n_estimators: Number of boosting rounds.\n",
    "# max_depth: Maximum depth of a tree.\n",
    "# gamma: Minimum loss reduction required to make a further partition on a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44.Describe the Process of Gradient Boosting in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Process of Gradient Boosting in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45.Advantages and Disadvantages of Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages:\n",
    "# High performance.\n",
    "# Ability to handle missing data.\n",
    "# Regularization to avoid overfitting.\n",
    "# Disadvantages:\n",
    "# Computationally expensive.\n",
    "# Requires tuning of many hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
